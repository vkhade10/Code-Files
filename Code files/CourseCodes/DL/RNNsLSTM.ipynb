{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jh5-KMT8f52R"
   },
   "source": [
    "# RNNs and LSTMs\n",
    "\n",
    "Build an RNN model to classify text and an LSTM model for anomaly detection (also outlier detection) on temperature sensor data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Obgo-lZd5Lzi"
   },
   "source": [
    "## Task 1: Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfrEGE2kiiqv"
   },
   "source": [
    "This task aims to train a sentiment analysis model to classify given sentences as **positive or negative**, based on the Recurrent Neural Network.\n",
    "\n",
    "---\n",
    "\n",
    "**Tasks**\n",
    "\n",
    "1. Load data\n",
    "2. Preprocess data\n",
    "3. Build RNN model\n",
    "4. Train model\n",
    "5. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zYDENWa4JZV"
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "# Extracting training data\n",
    "with open('task1_training_data.txt', encoding=\"utf8\") as file:\n",
    "    train_data = file.readlines()\n",
    "\n",
    "data_labels = []\n",
    "data_text = []\n",
    "for lines in train_data:\n",
    "    x = lines.split(' +++$+++ ')\n",
    "    data_labels.append(int(x[0]))\n",
    "    data_text.append(x[1])\n",
    "\n",
    "# Splitting data into training and validation sets\n",
    "l = int(0.8*len(data_text))\n",
    "train_text = data_text[:l]\n",
    "val_text = data_text[l:]\n",
    "train_labels = data_labels[:l]\n",
    "val_labels = data_labels[l:]\n",
    "    \n",
    "# Extracting test data\n",
    "with open('task1_test_data.txt', encoding=\"utf8\") as testfile:\n",
    "    test_data = testfile.readlines()\n",
    "\n",
    "test_text = []\n",
    "i = 0\n",
    "for lines in test_data[1:]:\n",
    "    x = lines.split(str(i)+',')\n",
    "    test_text.append(x[1])\n",
    "    i += 1\n",
    "\n",
    "print('First 10 examples from training set:\\n', train_text[:10])\n",
    "print('Labels for first 10 examples from training set:\\n', train_labels[:10])\n",
    "print('First 10 examples from test set:\\n', test_text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import numpy as np\n",
    "max_tokens = 2000\n",
    "max_len = 50\n",
    "\n",
    "# Text vectorization layer\n",
    "v_layer = TextVectorization(max_tokens = max_tokens, output_mode=\"int\",\n",
    "                            output_sequence_length = max_len)\n",
    "\n",
    "# Initializing the layer to create vocabulary\n",
    "v_layer.adapt(train_text)\n",
    "\n",
    "vocab = np.array(v_layer.get_vocabulary())\n",
    "print('First 20 tokens in vocabulary:\\n', vocab[:20])\n",
    "\n",
    "# Encoded data example\n",
    "ex_enc = v_layer(train_text).numpy()\n",
    "print(ex_enc[:10,:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Activation, Embedding, LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Building RNN model\n",
    "model = Sequential([\n",
    "    Input(shape=(1,), dtype=\"string\"),\n",
    "    v_layer,\n",
    "    Embedding(max_tokens + 1, 128),\n",
    "    LSTM(64),\n",
    "    Dense(64, activation = \"relu\"),\n",
    "    Dense(1, activation = \"sigmoid\")\n",
    "])\n",
    "\n",
    "#model.summary()\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model_history = model.fit(train_text, train_labels, epochs=10, batch_size=128, \n",
    "                          validation_data = (val_text, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plotting performance\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(model_history.history['accuracy'], c='b')\n",
    "plt.plot(model_history.history['val_accuracy'], c='r')\n",
    "plt.legend(['Training set', 'Test set'])\n",
    "plt.title('Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(model_history.history['loss'], c='b')\n",
    "plt.plot(model_history.history['val_loss'], c='r')\n",
    "plt.legend(['Training set', 'Test set'])\n",
    "plt.title('Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "predictions = model.predict(test_text)\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(\"\\nTest sentence: \", test_text[i])\n",
    "    print(\"\\nPredicted sentiment label value: \", predictions[i])\n",
    "    \n",
    "# Writing predicted data to csv file\n",
    "field = ['Predicted value', 'Sentence']\n",
    "row_data = []\n",
    "for j in range(0,len(test_text)):\n",
    "    row = [predictions[j], test_text[j]]\n",
    "    row_data.append(row)\n",
    "\n",
    "with open('ResultsCSV', 'w') as file:\n",
    "    write = csv.writer(file)\n",
    "    \n",
    "    write.writerow(field)\n",
    "    write.writerows(row_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnXTy7Xz5ScT"
   },
   "source": [
    "## Task 2: Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JR0YVG2sqTSV"
   },
   "source": [
    "In manufacturing industries, the anomaly detection technique is applied to predict the abnormal activities of machines based on the data read from sensors. In machine learning and data mining, anomaly detection is the task of identifying the rare items, events, or observations that are suspicious and seem different from the majority of the data. In this task, you will predict the possible failure of the system based on the temperature data. And this failure can be detected by check if they follow the trend of the majority of the data.\n",
    "\n",
    "---\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "The given dataset (`ambient_temperature_system_failure.csv`) is a part of Numenta Anomaly Benchmark (NAB) dataset, which is a novel benchmark for evaluating machine learning algorithms in anomaly detection.\n",
    "\n",
    "\n",
    "1. Load data\n",
    "2. Preprocess data\n",
    "3. Feature Engineering\n",
    "4. Prepare training and testing data\n",
    "5. Build LSTM model\n",
    "6. Train model\n",
    "5. Find anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jj58i84SAdmb"
   },
   "outputs": [],
   "source": [
    "# Write you code here\n",
    "\n",
    "# Required libraries\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Reading file - ambient_temperature_system_failure\n",
    "df = pd.read_csv(\"ambient_temperature_system_failure.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing time-series data\n",
    "figsize=(10,5)\n",
    "df.plot(x='timestamp', y='value', figsize=figsize, title='Temperature (F)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing data\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])  # converting timestamp data into datatime data\n",
    "df['value'] = (df['value'] - 32) * (5/9)  # converting temperature into celsius from fahrenheit\n",
    "df.plot(x='timestamp', y='value', figsize=figsize, title='Temperature (C)')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formating the data into required format\n",
    "df['hours'] = df['timestamp'].dt.hour\n",
    "df['dayornight'] = ((df['hours'] >= 7) & (df['hours'] <= 22)).astype(int)\n",
    "df['dayoftheweek'] = df['timestamp'].dt.dayofweek\n",
    "df['weekday'] = (df['dayoftheweek'] < 5).astype(int)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Normalizing data for LSTM model\n",
    "data = df[['value', 'hours', 'dayornight', 'dayoftheweek', 'weekday']]\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "d_scaled = min_max_scaler.fit_transform(data)\n",
    "data = pd.DataFrame(d_scaled)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8*len(data))\n",
    "\n",
    "# Training data\n",
    "x_train = data[0:train_size].values\n",
    "y_train = data[0:train_size][0].values\n",
    "\n",
    "# Test data\n",
    "x_test = data[train_size:].values\n",
    "y_test = data[train_size:][0].values\n",
    "\n",
    "# Defining sliding window function\n",
    "def sliding_window(tempdata, window_len=24):\n",
    "    res_data = []\n",
    "    for i in range(0, (len(tempdata)-window_len)):\n",
    "        res_data.append(data[i: i+window_len])\n",
    "    return np.asarray(res_data)\n",
    "\n",
    "# Preparing data using sliding window with window_length of 100\n",
    "win_l = 50\n",
    "x_train = sliding_window(x_train, win_l)\n",
    "y_train = y_train[-x_train.shape[0]:]\n",
    "x_test = sliding_window(x_test, win_l)\n",
    "y_test = y_test[-x_test.shape[0]:]\n",
    "\n",
    "# Shape of data\n",
    "print(\"x_train\", x_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"x_test\", x_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries to build LSTM\n",
    "from keras.layers.core import Dropout\n",
    "# other libraries already loaded\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(LSTM(50, input_dim = x_train.shape[-1], return_sequences=True))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(LSTM(100, return_sequences=False))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(1, activation = \"linear\"))\n",
    "\n",
    "# Model summary\n",
    "plot_model(model2, show_shapes=True)\n",
    "#model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling and fitting the model\n",
    "model2.compile(loss=\"mse\", optimizer=\"rmsprop\")\n",
    "\n",
    "# Train the model\n",
    "model2_history = model2.fit(x_train, y_train, epochs=20, batch_size=128, \n",
    "                          validation_data = (x_test, y_test))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(model2_history.history['loss'], c='b', label = 'training_loss')\n",
    "plt.plot(model2_history.history['val_loss'], c='r', label = 'test_loss')\n",
    "plt.title('Training Loss vs Test Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting predicted values vs real test values\n",
    "\n",
    "y_predict = model2.predict(x_test)\n",
    "plt.figure(figsize = (10,5))\n",
    "plt.plot(y_predict, c = 'r', label = 'Prediction on test data')\n",
    "plt.plot(y_test, c = 'b', label = 'Test values')\n",
    "plt.title('Real test values vs predicted values')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating threshold to detect anomalies\n",
    "diff = []\n",
    "for i in range(0, len(y_test)):\n",
    "    d = abs(y_test[i] - y_predict[i])\n",
    "    diff.append(d)\n",
    "\n",
    "diff = pd.Series(diff)\n",
    "outlier_fraction = 0.25\n",
    "n_outliers = int(outlier_fraction*len(diff))\n",
    "threshold = diff.astype(int).nlargest(n_outliers).min()\n",
    "\n",
    "# Detecting anomalies\n",
    "anomaly = (diff >= threshold).astype(int)\n",
    "anom_series = pd.Series(0, index = np.arange(len(x_train)))\n",
    "df['anomaly'] = anom_series.append(anomaly, ignore_index = 'True')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing anomalies (Red Dots)\n",
    "plt.figure(figsize=(15,7))\n",
    "a = df.loc[df['anomaly'] == 1, ['timestamp', 'value']] #anomaly\n",
    "plt.plot(df['timestamp'], df['value'], color='blue')\n",
    "plt.scatter(a['timestamp'],a['value'], color='red', label = 'Anomaly')\n",
    "#plt.axis([1.370*1e7, 1.405*1e7, 15,30])\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AuE8930 Mini Project 4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
